{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":77759,"sourceType":"datasetVersion","datasetId":339},{"sourceId":11295739,"sourceType":"datasetVersion","datasetId":7063219}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading and Configs","metadata":{"_uuid":"8a5babb3-6c9e-40ca-af85-966e21bf58de","_cell_guid":"eb6fb337-6558-486f-a72d-fa2afa63cd73","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nimport sys\nimport os\nimport gc\n\n# import ray\n# from ray import tune\n# from ray.tune.search.hyperopt import HyperOptSearch\n\n# ray.shutdown()  # engage new ray session\n# ray.init()\n\npd.set_option('display.max_rows', 100)","metadata":{"_uuid":"9ae51a96-bd51-4834-b8a3-b64b6d0032e5","_cell_guid":"09bf6a05-8bf6-4669-9486-bb8bd08dad55","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:48:40.843307Z","iopub.execute_input":"2025-04-06T11:48:40.843609Z","iopub.status.idle":"2025-04-06T11:48:44.938673Z","shell.execute_reply.started":"2025-04-06T11:48:40.843580Z","shell.execute_reply":"2025-04-06T11:48:44.935214Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir lstm_starter_v1","metadata":{"_uuid":"5705f5fb-cc60-4349-8568-0b81f9cf93fd","_cell_guid":"72a24265-dde9-48ee-86ee-cc94f38a48e6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:48:44.939754Z","iopub.execute_input":"2025-04-06T11:48:44.940247Z","iopub.status.idle":"2025-04-06T11:48:45.074840Z","shell.execute_reply.started":"2025-04-06T11:48:44.940216Z","shell.execute_reply":"2025-04-06T11:48:45.073582Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load data\ngenome_scores = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/genome_scores.csv\")\ngenome_tags = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/genome_tags.csv\")\nlinks = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/link.csv\")\nmovies = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/movie.csv\")\nrating = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/rating.csv\")\ntags = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/tag.csv\")","metadata":{"_uuid":"ef572e33-5edb-4833-a501-8e4667d7e657","_cell_guid":"1ca7822b-9c93-46cc-9e12-8bce98485186","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:48:45.076216Z","iopub.execute_input":"2025-04-06T11:48:45.076562Z","iopub.status.idle":"2025-04-06T11:49:08.646862Z","shell.execute_reply.started":"2025-04-06T11:48:45.076529Z","shell.execute_reply":"2025-04-06T11:49:08.646211Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IS_DEVELOP = True","metadata":{"_uuid":"b68ef1b3-d481-4a90-a36e-d1361a810620","_cell_guid":"b75df202-e495-4816-9573-15d2aefdd440","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:08.647659Z","iopub.execute_input":"2025-04-06T11:49:08.647895Z","iopub.status.idle":"2025-04-06T11:49:08.651610Z","shell.execute_reply.started":"2025-04-06T11:49:08.647875Z","shell.execute_reply":"2025-04-06T11:49:08.650696Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    # base settings\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    seed = 42\n    hyper_trials = 0\n\n    # train settings\n    seq_len = 10\n    sample_size = 100000 if IS_DEVELOP else 1000000\n    bs = 128\n    emb_dim = 128\n    hidden_dim = 256\n    lstm_layers = 2\n    dropout = 0.2\n    epochs = 3 if IS_DEVELOP else 5\n    lr = 1e-3\n    wd = 1e-2\n    top_k = 10\n    temperature = 0.85\n    candidate_pool_size = 100\n\n    #saving & inference\n    patience = 4\n    save_model_path = \"/kaggle/working/lstm_starter_v1/state.pth\"\n\n\n# seed everything\ndef set_seed(seed=73):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \nset_seed(CFG.seed)","metadata":{"_uuid":"df27fab9-e6c4-4399-b21e-466fefafc291","_cell_guid":"fbe3c479-62a9-4cad-8519-3d7d22723e9e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:08.652305Z","iopub.execute_input":"2025-04-06T11:49:08.652559Z","iopub.status.idle":"2025-04-06T11:49:08.719767Z","shell.execute_reply.started":"2025-04-06T11:49:08.652539Z","shell.execute_reply":"2025-04-06T11:49:08.719140Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# basic preprocess\ndf = genome_scores.copy()\ndf = df.merge(genome_tags, on = 'tagId', how = 'inner')\ndf = df.merge(links, on = 'movieId', how = 'inner')\ndf = df.merge(movies, on = 'movieId', how = 'inner')\nunique_movie_ids = df.movieId.unique()\n\ndel genome_scores, genome_tags, links, movies\ngc.collect()\n\nprint(\"PREPROCESS READY!!!\")","metadata":{"_uuid":"a10a4558-cfe7-417e-a840-4521f2ddc404","_cell_guid":"36e17ce9-5089-4d44-8466-d00a93851e8d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:08.722101Z","iopub.execute_input":"2025-04-06T11:49:08.722307Z","iopub.status.idle":"2025-04-06T11:49:12.172046Z","shell.execute_reply.started":"2025-04-06T11:49:08.722290Z","shell.execute_reply":"2025-04-06T11:49:12.171230Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# make a table\ndf['movieId'] = df['movieId'].astype(str)\nmovie_tag_df = df.pivot_table(index='movieId', columns='tag', values='relevance', fill_value=0)\n\n# only contain those that are present in movie data\nrating['movieId'] = rating['movieId'].astype(str)\nmovie_encoder = LabelEncoder()\nrating['movieId_enc'] = movie_encoder.fit_transform(rating['movieId'])\nmovie_tag_df = movie_tag_df.loc[movie_tag_df.index.intersection(movie_encoder.classes_)]\n\nnum_movies = len(movie_encoder.classes_)\n# print(num_movies) # 26744\n\nmovie_tag_tensor = torch.tensor(movie_tag_df.values, dtype=torch.float).to(CFG.device)  # (num_movies, tag_dim)\nraw2idx = {raw_id: i for i, raw_id in enumerate(movie_tag_df.index)}","metadata":{"_uuid":"ff723c45-d815-4978-8403-ccf894376938","_cell_guid":"ae18e875-3043-4602-87b1-d5d9121fca31","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:12.173249Z","iopub.execute_input":"2025-04-06T11:49:12.173457Z","iopub.status.idle":"2025-04-06T11:49:33.260698Z","shell.execute_reply.started":"2025-04-06T11:49:12.173439Z","shell.execute_reply":"2025-04-06T11:49:33.260039Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def batch_retrieve_candidate_pool(seq_raw_batch, rating_seq_batch, movie_tag_tensor, raw2idx, top_n):\n    \"\"\"\n    Parameters:\n      seq_raw_batch: list of lists of raw movie IDs, shape (B, seq_len)\n      rating_seq_batch: torch.Tensor of shape (B, seq_len)\n      movie_tag_tensor: torch.Tensor of shape (num_movies, tag_dim)\n      raw2idx: dictionary mapping raw movie id to row index in movie_tag_tensor\n      top_n: int, number of candidates to retrieve\n      \n    Returns:\n      candidate_indices: torch.Tensor of shape (B, top_n)\n    \"\"\"\n    device = rating_seq_batch.device\n    B = len(seq_raw_batch)\n    \n    # Convert raw movie ids to indices for each sample\n    indices = torch.tensor([[raw2idx[movie_id] for movie_id in seq] for seq in seq_raw_batch], device=device)\n    \n    # Gather tag vectors for each movie in each sequence: (B, seq_len, tag_dim)\n    seq_tags = movie_tag_tensor[indices]  # (B, seq_len, tag_dim)\n    \n    # Expand ratings to shape (B, seq_len, 1)\n    ratings = rating_seq_batch.unsqueeze(2)  # (B, seq_len, 1)\n    \n    # Compute weighted tag vectors: (B, seq_len, tag_dim)\n    weighted_tags = seq_tags * ratings\n    \n    # Sum over the sequence to form the user profile: (B, tag_dim)\n    user_profiles = weighted_tags.sum(dim=1)\n    \n    # Normalize user profiles: (B, tag_dim)\n    user_profiles = F.normalize(user_profiles, p=2, dim=1, eps=1e-8)\n    \n    # Normalize movie tag tensor (pre-compute norms): (num_movies, tag_dim)\n    movie_tag_norm = F.normalize(movie_tag_tensor, p=2, dim=1, eps=1e-8)\n    \n    # Compute cosine similarities: (B, num_movies)\n    sims = torch.matmul(user_profiles, movie_tag_norm.t())\n    \n    # Retrieve the top_n candidate movie indices for each sample.\n    _, candidate_indices = torch.topk(sims, k=top_n, dim=1)\n    \n    return candidate_indices  # (B, top_n)\n\n\ndef custom_collate(batch):\n    seq_movies = torch.stack([item[0] for item in batch])\n    seq_ratings = torch.stack([item[1] for item in batch])\n    tag_seq = torch.stack([item[2] for item in batch])\n    target = torch.stack([item[3] for item in batch])\n    seq_raw = [item[4] for item in batch]\n    return seq_movies, seq_ratings, tag_seq, target, seq_raw","metadata":{"_uuid":"c99381be-b82e-42e6-a114-e6220c5d7407","_cell_guid":"6b3eb06d-235b-4cd8-a5fb-2c2f7bc7a62c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:33.261556Z","iopub.execute_input":"2025-04-06T11:49:33.261854Z","iopub.status.idle":"2025-04-06T11:49:33.268839Z","shell.execute_reply.started":"2025-04-06T11:49:33.261826Z","shell.execute_reply":"2025-04-06T11:49:33.267797Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MovieRatingTagDataset(Dataset):\n    def __init__(self, rating_df, movie_tag_features, seq_len=5):\n        self.samples = []\n        # For each user, precompute candidate pools for each sequence.\n        for user_id, group in rating_df.groupby('userId'):\n            group = group.sort_values('timestamp')\n            movies_enc = group['movieId_enc'].tolist()\n            ratings_list = group['rating'].tolist()\n            movies_raw = group['movieId'].tolist() \n            for i in range(len(movies_enc) - seq_len):\n                seq_movies = movies_enc[i:i+seq_len]\n                seq_ratings = ratings_list[i:i+seq_len]\n                seq_raw = movies_raw[i:i+seq_len]\n                target = movies_enc[i+seq_len]\n                # check if all ids are in movie_tag table\n                if not all(r in movie_tag_features.index for r in seq_raw):\n                    continue\n                tag_seq = [movie_tag_features.loc[r].values for r in seq_raw]\n\n                self.samples.append((seq_movies, seq_ratings, tag_seq, target, seq_raw))\n                \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        seq_movies, seq_ratings, tag_seq, target, seq_raw = self.samples[idx]\n        tag_seq_array = np.array(tag_seq)\n        return (\n            torch.tensor(seq_movies, dtype=torch.long),   \n            torch.tensor(seq_ratings, dtype=torch.float),   \n            torch.tensor(tag_seq_array, dtype=torch.float), \n            torch.tensor(target, dtype=torch.long),         \n            seq_raw # raw movieId sequence (list of str)\n        )","metadata":{"_uuid":"92469a58-2da2-4993-aa73-19dcde602e48","_cell_guid":"805b4147-662b-4177-b6f5-912a5d703736","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:33.269743Z","iopub.execute_input":"2025-04-06T11:49:33.269987Z","iopub.status.idle":"2025-04-06T11:49:33.289266Z","shell.execute_reply.started":"2025-04-06T11:49:33.269968Z","shell.execute_reply":"2025-04-06T11:49:33.288463Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model\nclass MovieRatingTagLSTM(nn.Module):\n    def __init__(self, num_movies, tag_dim, emb_dim=64, hidden_dim=128, lstm_layers=4, dropout=0.3, proj_dim=128):\n        super().__init__()\n        self.movie_emb = nn.Embedding(num_movies, emb_dim)\n        self.rating_fc = nn.Linear(1, emb_dim)\n        self.tag_encoder = nn.Sequential(\n            nn.Linear(tag_dim, emb_dim),\n            nn.ReLU(),\n            nn.Linear(emb_dim, emb_dim)\n        )\n        input_dim = emb_dim * 3  # concatenated features: movie, rating, tag\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, dropout=dropout, bidirectional=True)\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=hidden_dim*2, num_heads=8, dropout=dropout)\n        self.fc1 = nn.Linear(hidden_dim * 2, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        # Project sequence representation into a space where we can compare via cosine similarity.\n        self.seq_projection = nn.Linear(512, proj_dim)\n        # We’ll use normalized movie embeddings (projected from self.movie_emb)\n        self.movie_projection = nn.Linear(emb_dim, proj_dim)\n        \n    def forward_embedding(self, movie_seq, rating_seq, tag_seq):\n        movie_vec = self.movie_emb(movie_seq)                  # (B, L, emb_dim)\n        rating_vec = self.rating_fc(rating_seq.unsqueeze(-1))    # (B, L, emb_dim)\n        tag_vec = self.tag_encoder(tag_seq)                      # (B, L, emb_dim)\n        x = torch.cat([movie_vec, rating_vec, tag_vec], dim=-1)    # (B, L, emb_dim*3)\n        lstm_out, _ = self.lstm(x)                               # (B, L, hidden_dim*2)\n        attn_output, _ = self.multihead_attn(lstm_out, lstm_out, lstm_out)\n        attn_output = torch.mean(attn_output, dim=1)             # (B, hidden_dim*2)\n        x = F.relu(self.fc1(attn_output))\n        x = F.relu(self.fc2(x))\n        seq_embed = self.seq_projection(x)                       # (B, proj_dim)\n        seq_embed = F.normalize(seq_embed, p=2, dim=1)\n        return seq_embed\n    \n    def get_movie_embeddings(self):\n        with torch.no_grad():\n            movie_embeds = self.movie_emb.weight              \n            proj_movie_embeds = self.movie_projection(movie_embeds)  # (num_movies, proj_dim)\n            proj_movie_embeds = F.normalize(proj_movie_embeds, p=2, dim=1)\n        return proj_movie_embeds","metadata":{"_uuid":"753dac46-97b3-401a-91d8-58d69c279a7f","_cell_guid":"93f05d53-81be-4a2f-bf1d-208f74c328c3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:33.290267Z","iopub.execute_input":"2025-04-06T11:49:33.290566Z","iopub.status.idle":"2025-04-06T11:49:33.314743Z","shell.execute_reply.started":"2025-04-06T11:49:33.290537Z","shell.execute_reply":"2025-04-06T11:49:33.314123Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train functions\ndef train_one_epoch_vectorized(model, optimizer, criterion, train_loader, device, top_n, movie_tag_tensor, raw2idx):\n    model.train()\n    total_loss = 0\n\n    # movie embeddings, computed once per training epoch\n    all_movie_embeds = model.get_movie_embeddings()  \n    \n    for movie_seq, rating_seq, tag_seq, target, seq_raw in tqdm(train_loader, desc=f\"Train Epoch\"):\n        movie_seq = movie_seq.to(device)\n        rating_seq = rating_seq.to(device) \n        tag_seq = tag_seq.to(device)\n        target = target.to(device)\n        \n        # retrieve batch-wise candidate pool.\n        candidate_pools = batch_retrieve_candidate_pool(seq_raw, rating_seq, movie_tag_tensor, raw2idx, top_n).to(device)\n\n        # get embeddings\n        optimizer.zero_grad()\n        seq_embed = model.forward_embedding(movie_seq, rating_seq, tag_seq)  # (B, proj_dim)\n        candidate_embeds = all_movie_embeds[candidate_pools]  # (B, top_n, proj_dim)\n        \n        # compute similarity\n        seq_embed_expanded = seq_embed.unsqueeze(1)  # (B, 1, proj_dim)\n        batch_logits = torch.bmm(seq_embed_expanded, candidate_embeds.transpose(1, 2)).squeeze(1)  # (B, top_n)\n        \n        # check if target is in candidate pool\n        eq = (candidate_pools == target.unsqueeze(1))  # (B, top_n)\n        target_mask = eq.any(dim=1)  # (B,)\n\n        # if not skip\n        if target_mask.sum() == 0:\n            continue \n\n        # Filter valid samples\n        train_logits = batch_logits[target_mask]\n        train_eq = eq[target_mask]\n        train_targets = train_eq.float().argmax(dim=1)\n\n        # apply temperature scaling\n        temperature = CFG.temperature\n        train_logits = train_logits / temperature\n        \n        # compute loss and accuracy on valid samples only\n        loss = criterion(train_logits, train_targets)\n        total_loss += loss.item() * train_logits.size(0)\n        loss.backward(retain_graph=True)\n        optimizer.step()\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f\"Train Loss: {avg_loss:.4f}\")\n    return avg_loss\n\n\ndef validate_vectorized(model, criterion, valid_loader, device, top_n, movie_tag_tensor, raw2idx, top_k=5):\n    model.eval()\n    total_loss = 0\n    total = 0\n    correct_topk = 0\n    skipped = 0 \n    \n    # movie embeddings, computed once per validation epoch\n    all_movie_embeds = model.get_movie_embeddings()  # (num_movies, proj_dim)\n    \n    with torch.no_grad():\n        for movie_seq, rating_seq, tag_seq, target, seq_raw in tqdm(valid_loader, desc=\"Valid Epoch\"):\n            movie_seq = movie_seq.to(device)\n            rating_seq = rating_seq.to(device)\n            tag_seq = tag_seq.to(device)\n            target = target.to(device)\n            \n            # compute candidate pools\n            candidate_pools = batch_retrieve_candidate_pool(seq_raw, rating_seq, movie_tag_tensor, raw2idx, top_n).to(device)  # (B, top_n)\n\n            # Embeddings\n            seq_embed = model.forward_embedding(movie_seq, rating_seq, tag_seq)  # (B, proj_dim)\n            candidate_embeds = all_movie_embeds[candidate_pools]  # (B, top_n, proj_dim)\n            batch_logits = torch.bmm(seq_embed.unsqueeze(1), candidate_embeds.transpose(1, 2)).squeeze(1)  # (B, top_n)\n            \n            # Mask: check if target is in candidate pool\n            eq = (candidate_pools == target.unsqueeze(1))  # (B, top_n)\n            target_mask = eq.any(dim=1)  # (B,)\n            if target_mask.sum() == 0:\n                skipped += target.size(0)\n                continue  # Skip this batch completely if no valid target\n            \n            # filter\n            valid_logits = batch_logits[target_mask]  \n            valid_eq = eq[target_mask]                \n            # this gives the index in the candidate pool where the true target is located\n            valid_targets = valid_eq.float().argmax(dim=1)  \n\n            # loss\n            loss = criterion(valid_logits, valid_targets)\n            total_loss += loss.item() * valid_logits.size(0)\n            \n            # top-k predictions\n            topk_indices = valid_logits.topk(k=top_k, dim=1, largest=True, sorted=True)[1]\n            correct_topk += (topk_indices == valid_targets.unsqueeze(1)).any(dim=1).float().sum().item()\n            \n            total += valid_logits.size(0)\n    \n    avg_loss = total_loss / total if total > 0 else 0.0\n    topk_accuracy = correct_topk / total if total > 0 else 0.0\n    coverage = total / (total + skipped) if (total + skipped) > 0 else 0.0\n    \n    print(f\"Valid Loss: {avg_loss:.4f} | Top-{top_k} Accuracy: {topk_accuracy * 100:.2f}% | Coverage: {coverage:.4f}\")\n    return avg_loss, topk_accuracy","metadata":{"_uuid":"509ceaa8-68c2-4f00-a692-0f3b367b2c3f","_cell_guid":"75b38968-0a8e-4ec2-880e-cb4344d623e4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:33.315488Z","iopub.execute_input":"2025-04-06T11:49:33.315792Z","iopub.status.idle":"2025-04-06T11:49:33.334604Z","shell.execute_reply.started":"2025-04-06T11:49:33.315761Z","shell.execute_reply":"2025-04-06T11:49:33.333960Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data preparation\n\n# sample out part of the dataset\n# rating_df = rating.sample(n=CFG.sample_size, random_state=CFG.seed)\n\nimport random\nstart_idx = random.randint(0, len(rating) - CFG.sample_size)\nrating_df = rating.iloc[start_idx:start_idx + CFG.sample_size]\n\n\n# stratified split to ensure the actual ratio for train, valid, test is 0.8 : 0.1 : 0.1\nuser_activity = rating_df['userId'].value_counts().rename('count').reset_index()\nuser_activity.columns = ['userId', 'count']\nuser_activity['activity_bin'] = pd.qcut(user_activity['count'], q=4, labels=False, duplicates=\"drop\")\ntrain_users, test_users = train_test_split(\n    user_activity['userId'],\n    test_size=0.2,\n    stratify=user_activity['activity_bin'],\n    random_state=42\n)\n\ntest_user_bins = user_activity.set_index('userId').loc[test_users]['activity_bin']\nvalid_users, test_users = train_test_split(\n    test_users,\n    test_size=0.5,\n    stratify=test_user_bins,\n    random_state=42\n)\n\n# indexing\ntrain_rating = rating_df[rating_df['userId'].isin(train_users)].copy()\nvalid_rating = rating_df[rating_df['userId'].isin(valid_users)].copy()\ntest_rating = rating_df[rating_df['userId'].isin(test_users)].copy()\n\n# datasets\ntrain_dataset = MovieRatingTagDataset(train_rating, movie_tag_df, seq_len=CFG.seq_len)\nvalid_dataset = MovieRatingTagDataset(valid_rating, movie_tag_df, seq_len=CFG.seq_len)\ntest_dataset = MovieRatingTagDataset(test_rating, movie_tag_df, seq_len=CFG.seq_len)\n\n# loaders\ntrain_loader = DataLoader(train_dataset, batch_size=CFG.bs, shuffle=True, collate_fn=custom_collate)\nvalid_loader = DataLoader(valid_dataset, batch_size=CFG.bs, shuffle=False, collate_fn=custom_collate)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.bs, shuffle=False, collate_fn=custom_collate)\n\nprint(\"DATALOADER READY!!!\")","metadata":{"_uuid":"79d647a8-1b74-4215-9cf0-1fe14e76c8c7","_cell_guid":"6d236e9d-575f-4b07-8261-468c4c7b9e04","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:33.335445Z","iopub.execute_input":"2025-04-06T11:49:33.335694Z","iopub.status.idle":"2025-04-06T11:49:52.883872Z","shell.execute_reply.started":"2025-04-06T11:49:33.335668Z","shell.execute_reply":"2025-04-06T11:49:52.883088Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# search_space = {\n#     \"temperature\": tune.uniform(0.5, 1.0),\n#     \"lr\": tune.loguniform(1e-5, 1e-3),\n#     \"emb_dim\": tune.choice([64, 128]),\n#     \"hidden_dim\": tune.choice([128, 256]),\n#     \"lstm_layers\": tune.choice([2, 4]),  # If you increase seq_len later, you might try more layers.\n#     \"dropout\": tune.uniform(0.2, 0.5),\n#     \"wd\": tune.loguniform(1e-5, 1e-3),\n#     \"epochs\": tune.choice([10, 15, 20]),\n#     \"candidate_pool_size\": tune.choice([300, 500, 1000]),\n#     \"label_smoothing\": tune.uniform(0, 0.2),\n# }\n\n# movie_tag_tensor_ref = ray.put(movie_tag_tensor)\n# raw2idx_ref = ray.put(raw2idx)\n# tuned_train_model = tune.with_parameters(\n#     train_model,  # your training function defined earlier\n#     movie_tag_tensor=movie_tag_tensor_ref,\n#     raw2idx=raw2idx_ref,\n#     train_loader=train_loader,  # assuming these are defined globally or similarly passed\n#     valid_loader=valid_loader\n# )\n\n# def train_model(config):\n#     CFG.temperature = config[\"temperature\"]\n    \n#     # Create the model with hyperparameters from config.\n#     model = MovieRatingTagLSTM(\n#         num_movies=26744,\n#         tag_dim=movie_tag_tensor.shape[1],\n#         emb_dim=config[\"emb_dim\"],\n#         hidden_dim=config[\"hidden_dim\"],\n#         lstm_layers=config[\"lstm_layers\"],\n#         dropout=config[\"dropout\"],\n#         proj_dim=128  # You can also tune this if needed.\n#     ).to(CFG.device)\n    \n#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"wd\"])\n#     criterion = nn.CrossEntropyLoss(label_smoothing=config[\"label_smoothing\"])\n    \n#     # For simplicity, we run for a fixed number of epochs.\n#     for epoch in range(config[\"epochs\"]):\n#         loss = train_one_epoch_vectorized(\n#             model, optimizer, criterion, train_loader,\n#             device, config[\"candidate_pool_size\"], movie_tag_tensor, raw2idx, config\n#         )\n#         # Here you could add a validation loop and report validation metrics.\n#         # For now, we report training loss.\n#         tune.report(loss=loss)","metadata":{"_uuid":"8b4dbb83-4dd1-482d-af26-883fc4a4b24c","_cell_guid":"775d3d81-3dd9-4d15-aa45-f5ea33b9e51a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:52.935721Z","iopub.status.idle":"2025-04-06T11:49:52.935979Z","shell.execute_reply":"2025-04-06T11:49:52.935875Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training block\ntag_dim = movie_tag_df.shape[1]\n\n# num_movies, tag_dim, num_users=None, emb_dim=128, hidden_dim=512, lstm_layers=3, dropout=0.3\nmodel = MovieRatingTagLSTM(\n    num_movies=num_movies, \n    tag_dim=tag_dim, emb_dim=CFG.emb_dim, \n    hidden_dim=CFG.hidden_dim, \n    lstm_layers=CFG.lstm_layers,\n    dropout=CFG.dropout,\n).to(CFG.device)\nprint(\"MODEL READY!!!\")\n\n# training utils\ncriterion = nn.CrossEntropyLoss(label_smoothing = 0.1)\noptimizer = optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\nscheduler = CosineAnnealingLR(optimizer, T_max=CFG.epochs, eta_min=1e-5)\n\n# record training results\ntrain_losses, valid_losses, valid_accs = [], [], []\nbest_acc = 0\npatience_count = 0\nbest_model_state = model.state_dict()\n\nif not CFG.hyper_trials:\n    for epoch in range(CFG.epochs):\n        tloss = train_one_epoch_vectorized(model, optimizer, criterion, train_loader, CFG.device, CFG.candidate_pool_size, movie_tag_tensor, raw2idx)\n        vloss, vacc = validate_vectorized(model, criterion, valid_loader, CFG.device, CFG.candidate_pool_size, movie_tag_tensor, raw2idx, CFG.top_k)\n        train_losses.append(tloss)\n        valid_losses.append(vloss)\n        valid_accs.append(vacc)\n        \n        if vacc > best_acc:\n            best_acc = vacc\n            best_model_state = model.state_dict()\n            patience_count = 0\n            torch.save(best_model_state, CFG.save_model_path)\n            print(\"Best model state updated.\")\n        else:\n            patience_count += 1\n            if patience_count >= CFG.patience:\n                print(\"Early stopping triggered\")\n                break\n        scheduler.step()\n    \n    model.load_state_dict(best_model_state)\n    torch.save(best_model_state, CFG.save_model_path)\n    \nelse:\n    analysis = tune.run(\n        tuned_train_model,\n        config=search_space,\n        num_samples=CFG.hyper_trials,\n        resources_per_trial={\"cpu\": 4, \"gpu\": 1}, \n        metric=\"loss\",\n        mode=\"min\",\n        progress_reporter=tune.CLIReporter(metric_columns=[\"loss\", \"accuracy\"])\n    )\n    \n    print(\"Best hyperparameters found were: \", analysis.best_config)\n    print(\"Best result:\", analysis.best_result)","metadata":{"_uuid":"0f1e96c3-7c30-40ff-9e21-f428dbff840f","_cell_guid":"9a76657a-4ead-4ff4-8136-1ebcf4fc7798","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:52.936821Z","iopub.status.idle":"2025-04-06T11:49:52.937223Z","shell.execute_reply":"2025-04-06T11:49:52.937038Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize valid accuracy\nif not CFG.hyper_trials:\n    plt.figure(figsize=(6, 4))\n    plt.plot(valid_accs, marker='o', linestyle='-', color='b', label='Eval Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Evaluation Accuracy over Epochs')\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"6086664d-f47e-4ac9-bf25-c5a0c76b8b6e","_cell_guid":"fa4ab34e-4345-49cd-99f3-32b379bbbcd1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:52.938262Z","iopub.status.idle":"2025-04-06T11:49:52.938665Z","shell.execute_reply":"2025-04-06T11:49:52.938478Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize train loss\nif not CFG.hyper_trials:\n    plt.figure(figsize=(6, 4))\n    plt.plot(train_losses, marker='o', linestyle='-', color='b', label='Train Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Train Loss')\n    plt.title('Train Loss over Epochs')\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"472414de-79f3-446c-85aa-29fad1d073af","_cell_guid":"225ad202-015f-4e6d-8170-61ffe63c380f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:52.939696Z","iopub.status.idle":"2025-04-06T11:49:52.940100Z","shell.execute_reply":"2025-04-06T11:49:52.939910Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# visualize valid loss\nif not CFG.hyper_trials:\n    plt.figure(figsize=(6, 4))\n    plt.plot(valid_losses, marker='o', linestyle='-', color='b', label='Train Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Valid Loss')\n    plt.title('Valid Loss over Epochs')\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"80a99e5b-84d9-4645-a844-d39112963128","_cell_guid":"b5b9e13f-0a51-4155-935b-92796db5c850","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:49:52.940891Z","iopub.status.idle":"2025-04-06T11:49:52.941279Z","shell.execute_reply":"2025-04-06T11:49:52.941116Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference","metadata":{"_uuid":"8645ada8-4ce6-4bee-bbff-db626d3d72cf","_cell_guid":"58a2ecb9-0e62-4962-af55-62bce3e4044b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def compute_top_k_accuracy(predictions, targets, k=5):\n    correct = 0\n    total = 0\n\n    for pred, target in zip(predictions, targets):\n        # Check if the target is in the top-k predictions\n        # Flatten the arrays in case they are not 1D\n        pred = pred.flatten()\n        target = target.flatten()\n\n        # Check if the target is in the top-K predictions\n        correct += (pred[:k] == target).sum().item()\n        total += 1\n\n    return correct / total if total > 0 else 0.0\n\n\ndef infer_with_test_loader(model, test_loader, device, top_n, movie_tag_tensor, raw2idx, return_top_k=5):\n    model.eval()\n\n    all_predictions = []\n    all_targets = []\n    skipped = 0  # Track skipped batches where no valid target is found\n\n    with torch.no_grad():\n        for movie_seq, rating_seq, tag_seq, target, seq_raw in tqdm(test_loader, desc=\"Inference\"):\n            # Move inputs to device\n            movie_seq = movie_seq.to(device)\n            rating_seq = rating_seq.to(device)\n            tag_seq = tag_seq.to(device)\n            target = target.to(device)\n\n            # Precompute all movie embeddings once\n            all_movie_embeds = model.get_movie_embeddings()  # (num_movies, proj_dim)\n            \n            # Compute candidate pools for the batch\n            candidate_pools = batch_retrieve_candidate_pool(seq_raw, rating_seq, movie_tag_tensor, raw2idx, top_n).to(device)\n\n            # Embeddings\n            seq_embed = model.forward_embedding(movie_seq, rating_seq, tag_seq)  # (B, proj_dim)\n            candidate_embeds = all_movie_embeds[candidate_pools]  # (B, top_n, proj_dim)\n            batch_logits = torch.bmm(seq_embed.unsqueeze(1), candidate_embeds.transpose(1, 2)).squeeze(1)  # (B, top_n)\n            \n            # Mask: check if target is in candidate pool\n            eq = (candidate_pools == target.unsqueeze(1))  # (B, top_n)\n            target_mask = eq.any(dim=1)  # (B,)\n\n            if target_mask.sum() == 0:\n                skipped += target.size(0)\n                continue  # Skip this batch completely if no valid target\n\n            # Filter valid samples\n            valid_logits = batch_logits[target_mask]\n            valid_eq = eq[target_mask]\n            valid_targets = valid_eq.float().argmax(dim=1)\n\n            # Get the top-K predicted movie IDs\n            topk_scores, topk_movies = valid_logits.topk(return_top_k, dim=1, largest=True, sorted=True)\n\n            # Store predictions and targets\n            all_predictions.append(topk_movies.cpu().numpy())  # List of top-k movie indices (for each user)\n            all_targets.append(valid_targets.cpu().numpy())  # Correct target for each user\n\n    # Compute top-K accuracy\n    accuracy = compute_top_k_accuracy(np.concatenate(all_predictions), np.concatenate(all_targets), k=return_top_k)\n    \n    return all_predictions, all_targets, accuracy","metadata":{"_uuid":"75248ae1-62e2-4ace-8216-8e3991d09058","_cell_guid":"b11a1263-282f-4600-9a3e-10ad90e14ed5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:51:52.499287Z","iopub.execute_input":"2025-04-06T11:51:52.499700Z","iopub.status.idle":"2025-04-06T11:51:52.508666Z","shell.execute_reply.started":"2025-04-06T11:51:52.499674Z","shell.execute_reply":"2025-04-06T11:51:52.507693Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tag_dim = movie_tag_df.shape[1]\nmodel = MovieRatingTagLSTM(\n    num_movies=num_movies, \n    tag_dim=tag_dim, emb_dim=CFG.emb_dim, \n    hidden_dim=CFG.hidden_dim, \n    lstm_layers=CFG.lstm_layers,\n    dropout=CFG.dropout,\n).to(CFG.device)\n\nmodel.load_state_dict(torch.load(\"/kaggle/input/movie-rec-lstm-v1/lstm_starter_v1/state.pth\", map_location = CFG.device))","metadata":{"_uuid":"48429c33-18e7-4838-aec0-10bff97a843a","_cell_guid":"026a2170-80d4-4d87-8716-6774bfdeeb00","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:51:54.897590Z","iopub.execute_input":"2025-04-06T11:51:54.897879Z","iopub.status.idle":"2025-04-06T11:51:55.299887Z","shell.execute_reply.started":"2025-04-06T11:51:54.897858Z","shell.execute_reply":"2025-04-06T11:51:55.299076Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not CFG.hyper_trials:\n    top_n = 100\n    raw2idx = raw2idx  \n    return_top_k = 1\n    \n    # Run inference with the test loader and compute accuracy\n    predictions, targets, accuracy = infer_with_test_loader(\n        model, test_loader, CFG.device, top_n, movie_tag_tensor, raw2idx, return_top_k\n    )\n    \n    print(f\"Top-{return_top_k} Accuracy: {accuracy * 100:.2f}%\")","metadata":{"_uuid":"20cf182e-d752-40d1-85d3-43ed6a5835a8","_cell_guid":"46b36115-697a-43f8-a064-8671c028e7e1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-06T11:58:33.176314Z","iopub.execute_input":"2025-04-06T11:58:33.176611Z","iopub.status.idle":"2025-04-06T11:58:34.322264Z","shell.execute_reply.started":"2025-04-06T11:58:33.176588Z","shell.execute_reply":"2025-04-06T11:58:34.321178Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"also need to build the inference cycle for raw inputs","metadata":{"_uuid":"b3e06114-5fd5-4f3c-8420-db937294c4c9","_cell_guid":"be7d5754-e371-480c-9f63-cea5f04ca9f0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}